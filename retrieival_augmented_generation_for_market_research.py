# -*- coding: utf-8 -*-
"""Retrieival-Augmented Generation for Market Research.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19xUKte_7oiqBJsFpkrIKcGlt6TDqo-Wo
"""

import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
import torch
from transformers import DPRContextEncoderTokenizer, DPRContextEncoder, RagTokenizer, RagRetriever, RagSequenceForGeneration, BartForConditionalGeneration
from flask import Flask, request, jsonify
from sklearn.feature_extraction.text import TfidfVectorizer

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load and preprocess data
df1 = pd.read_csv("cnbc_headlines.csv")
df2 = pd.read_csv("guardian_headlines.csv")
df3 = pd.read_csv("reuters_headlines.csv")
dfs = [df1, df2, df3]
df_combined = pd.concat(dfs, ignore_index=True)

# Define stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Function to preprocess text
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenization
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize
    return ' '.join(tokens)

# Apply preprocessing to headlines
df_combined['cleaned_headline'] = df_combined['Headlines'].fillna('').apply(preprocess_text)
df_combined['Time'] = pd.to_datetime(df_combined['Time'], errors='coerce')

# Handle missing values
df_combined.dropna(subset=['Headlines', 'Time'], inplace=True)
df_combined['Description'].fillna('No description available', inplace=True)

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=1000)
tfidf_matrix = vectorizer.fit_transform(df_combined['cleaned_headline'])
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Set up DPR context encoder and tokenizer
context_encoder_name = 'facebook/dpr-ctx_encoder-single-nq-base'
tokenizer_dpr = DPRContextEncoderTokenizer.from_pretrained(context_encoder_name)
context_encoder = DPRContextEncoder.from_pretrained(context_encoder_name)

# Set up RAG tokenizer, retriever, and generation model
tokenizer_rag = RagTokenizer.from_pretrained("facebook/rag-token-nq")
retriever = RagRetriever.from_pretrained("facebook/rag-token-nq", index_name="exact", use_dummy_dataset=True)
generator = RagSequenceForGeneration.from_pretrained("facebook/rag-token-nq", retriever=retriever, tokenizer="facebook/bart-large-cnn")

# Function for generating summaries using RAG
def generate_summary_rag(query):
    inputs = tokenizer_rag(query, return_tensors="pt")
    with torch.no_grad():
        outputs = generator.generate(inputs["input_ids"])
    return tokenizer_rag.decode(outputs[0], skip_special_tokens=True)

# Function for retrieving documents using DPR
def retrieve(query, df, tokenizer, context_encoder, top_k=5):
    inputs = tokenizer(query, padding='longest', truncation=True, return_tensors='pt')
    with torch.no_grad():
        embeddings = context_encoder(**inputs).pooler_output
    similarity_scores = torch.matmul(embeddings, embeddings.T).squeeze(0)
    top_indices = similarity_scores.argsort(descending=True)[:top_k]
    return df.iloc[top_indices]

# Test the retrieval and summarization using RAG
query = "impact of interest rates on stock market"
retrieved_docs = retrieve(query, df_combined, tokenizer_dpr, context_encoder)
for i, row in retrieved_docs.iterrows():
    headline = row['Headlines']
    summary = generate_summary_rag(headline)
    print(f"Headline: {headline}")
    print(f"Summary: {summary}")
    print()

